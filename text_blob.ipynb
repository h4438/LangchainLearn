{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d2b5d1-c1a2-48e2-97b3-2506999d649a",
   "metadata": {},
   "source": [
    "# Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac13793f-d4f8-4543-abfc-5c49f5ae9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "userText = \"Shut up! You are an idiot\"\n",
    "tb = TextBlob(userText)\n",
    "subjectivity = tb.subjectivity\n",
    "polarity = round(tb.polarity, 2)\n",
    "print(polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1113c5-ab88-4371-9ce7-77fd6a8e3d43",
   "metadata": {},
   "source": [
    "# 1 try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86d7234-3a13-49b5-8140-ac909398d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI ready\n",
      "Enable tracing at text_blob\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from setup import get_openai_model, enable_tracing\n",
    "\n",
    "MODEL = get_openai_model()\n",
    "enable_tracing(\"text_blob\")\n",
    "\n",
    "\n",
    "# define the output\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"sentiment\", description=\"a sentiment label based on the user text. It should be either Negative, Positive or Neutral\"),\n",
    "    ResponseSchema(name=\"reason\", description=\"\"\"\n",
    "    If the sentiment is Negative then return the reason why the user shouldn't have said that.\n",
    "    If the sentiment is Positive then return a compliment.\n",
    "    For Neutral then return a instruct for a better message. \n",
    "    \"\"\"),\n",
    "    ResponseSchema(name=\"reply\", description=\"the best and friendliest replacement to the given user text\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# prompt template\n",
    "template = \"\"\"You are good at detecting human emotion. All emotions you know are Negative, Positive and Neutral.\n",
    "Given a human text, subjectivity and polarity, your job is to answer as best as possible.\n",
    "Know that subjectivity is a measure of how much of a text is based on personal opinions or beliefs, rather than on facts. \n",
    "It is a float value between 0 and 1, where 0 represents an objective text and 1 represents a highly subjective text.\n",
    "Also know that polarity is a indicator for the sentiment of the given user text, negative value means Negative, positive value means Positive and 0 means Neutral.\n",
    "{format_instructions}\n",
    "User text: {text}\n",
    "Subjectivity: {subjectivity}\n",
    "Polarity: {polarity}\"\"\"\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\",\"subjectivity\",\"polarity\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "# Build chain\n",
    "sentiment_chain = LLMChain(llm=MODEL, prompt=prompt, output_key='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efad7591-eb0f-4867-b487-55e32050ccc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Have you heard about the new school',\n",
       " 'polarity': 0.14,\n",
       " 'subjectivity': 0.45454545454545453,\n",
       " 'result': '285714285714285\\n\\n```json\\n{\\n\\t\"sentiment\": \"Neutral\",\\n\\t\"reason\": \"This statement is too vague and does not provide enough information.\",\\n\\t\"reply\": \"What do you know about the new school?\"\\n}\\n```'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "userText = \"Have you heard about the new school\"\n",
    "tb = TextBlob(userText)\n",
    "subjectivity = tb.subjectivity\n",
    "polarity = round(tb.polarity, 2)\n",
    "\n",
    "ans = sentiment_chain({\"text\": userText, \"polarity\": polarity, \"subjectivity\": subjectivity})\n",
    "# ans = sentiment_chain.predict_and_parse(text=userText, polarity=polarity, subjectivity=subjectivity)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f140fc8-79fc-41ce-8089-308852d59b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'Neutral',\n",
       " 'reason': 'This statement is too vague and does not provide enough information.',\n",
       " 'reply': 'What do you know about the new school?'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = output_parser.parse(ans['result'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2a14f-f3a7-4dcb-8607-c9de776a99cf",
   "metadata": {},
   "source": [
    "# 2 try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a017734a-42bc-4150-bb55-c2152fa9a848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI ready\n",
      "Enable tracing at text_blob\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from setup import get_openai_model, enable_tracing\n",
    "\n",
    "MODEL = get_openai_model()\n",
    "enable_tracing(\"text_blob\")\n",
    "\n",
    "\n",
    "\n",
    "def parse_nested_json(text):\n",
    "   \n",
    "    a = text.strip()\n",
    "    json_data = a.strip().replace('```json', '').strip()\n",
    "    json_data = json_data.strip().replace('```', '').strip()\n",
    "    data = json.loads(json_data)\n",
    "    return data    \n",
    "\n",
    "response_schemas = []\n",
    "emos = ['Happy ðŸ˜Š','Sad ðŸ˜”','Angry ðŸ˜ ','Surprise ðŸ˜²','Fear ðŸ˜¨']\n",
    "# emos = ['Happy Happy','Sad Sad','Angry Angry','Surprise Surprise','Fear Fear']\n",
    "\n",
    "for emo in emos:\n",
    "    emos = emo.split(\" \")\n",
    "    # des = f\"\"\"a js array contains 3 elements in this order:\n",
    "    # 1. always return '{emos[1]}' \n",
    "    # 2. a emotional score from 1 to 100 for the {emos[0]} based the given user text\n",
    "    # 3. a reason for the score\"\"\"\n",
    "    des = f\"\"\"a js object contains 3 properties:\n",
    "    \"label\": str // always return '{emos[1]}' \n",
    "    \"score\": int // an emotional score from 1 to 100 for the {emos[0]}ness based the given user text\n",
    "    \"reason\": str// a reason for the score\"\"\"\n",
    "    # print(des)\n",
    "    schema = ResponseSchema(name=emos[0], description=des)\n",
    "    response_schemas.append(schema)\n",
    "# define the output\n",
    "\n",
    "output_icon_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# prompt template\n",
    "template = \"\"\"You are good at detecting human emotion, so good that you can measure the percentages of each emotion in Happy, Sad, Angry, Surprise, Fear from a given text. \n",
    "The sum of the percentages of each emotion in Happy, Sad, Angry, Surprise, Fear must be 100.\n",
    "Given an user text, your job is to answer as best as possible.\n",
    "{format_instructions}\n",
    "User text: {text}. This is the end of the user text.\"\"\"\n",
    "\n",
    "format_instructions = output_icon_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "# Build chain\n",
    "sentiment_icon_chain = LLMChain(llm=MODEL, prompt=prompt, output_key='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b45b5fa-484b-4560-bddc-c4d2c0199dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"You a bad guy. I don't want to see you again\",\n",
       " 'result': '\\n\\n```json\\n{\\n\\t\"Happy\": {\\n        \"label\": \"ðŸ˜Š\",\\n        \"score\": 0,\\n        \"reason\": \"The user text contains no positive sentiment.\"\\n    },\\n\\t\"Sad\": {\\n        \"label\": \"ðŸ˜”\",\\n        \"score\": 100,\\n        \"reason\": \"The user text contains negative sentiment.\"\\n    },\\n\\t\"Angry\": {\\n        \"label\": \"ðŸ˜ \",\\n        \"score\": 100,\\n        \"reason\": \"The user text contains negative sentiment.\"\\n    },\\n\\t\"Surprise\": {\\n        \"label\": \"ðŸ˜²\",\\n        \"score\": 0,\\n        \"reason\": \"The user text contains no surprise sentiment.\"\\n    },\\n\\t\"Fear\": {\\n        \"label\": \"ðŸ˜¨\",\\n        \"score\": 0,\\n        \"reason\": \"The user text contains no fear sentiment.\"\\n    }\\n}\\n```'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = sentiment_icon_chain({\"text\": \"You a bad guy. I don't want to see you again\"})\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6dabbae-a214-4cf1-ae93-fcbf052b72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Happy': {'label': 'ðŸ˜Š',\n",
       "  'score': 0,\n",
       "  'reason': 'The user text contains no positive sentiment.'},\n",
       " 'Sad': {'label': 'ðŸ˜”',\n",
       "  'score': 100,\n",
       "  'reason': 'The user text contains negative sentiment.'},\n",
       " 'Angry': {'label': 'ðŸ˜ ',\n",
       "  'score': 100,\n",
       "  'reason': 'The user text contains negative sentiment.'},\n",
       " 'Surprise': {'label': 'ðŸ˜²',\n",
       "  'score': 0,\n",
       "  'reason': 'The user text contains no surprise sentiment.'},\n",
       " 'Fear': {'label': 'ðŸ˜¨',\n",
       "  'score': 0,\n",
       "  'reason': 'The user text contains no fear sentiment.'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = parse_nested_json(ans['result'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25973946-3185-448a-80db-f30f525341a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
